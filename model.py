# -*- coding: utf-8 -*-
"""Project_V_1.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KDWRyb4iaUU8M3n11ylQAL_XCkurrXnt

> Load the Drive helper and mount

---
"""

# Commented out IPython magic to ensure Python compatibility.
# Load the Drive helper and mount.
from google.colab import drive

# Loading libraries
from sklearn.preprocessing import LabelEncoder, OneHotEncoder # used for encoding categorical data
from sklearn.preprocessing import StandardScaler # used for feature scaling
from sklearn.compose import ColumnTransformer
# %matplotlib inline
from matplotlib import pyplot as plt
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import StandardScaler
from scipy import stats
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE 
from sklearn.linear_model import LogisticRegression 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report 

import pandas as pd # used for handling dataset
import numpy as np # used for handling numbers
import seaborn as sns 
import warnings
warnings.filterwarnings("ignore")
print("done importing")


#loading the dataset

rawdata = pd.read_csv("C:/Users/Ashmita Agrawal/Downloads/Insurance Dataset.csv")

#rawdata = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/P42- Health Insurance/dataset/Insurance Dataset.csv")

#setting the output screen to display all columns and viewing the records
pd.set_option('display.max_columns', None)

rawdata.head()

rawdata.tail()

# Display the data types of columns
rawdata.info()

# Display basic statistics 
rawdata.describe()

rawdata.describe(include=np.object)

#checking the count value for each column
for column in rawdata.columns:
     print("\n" + column)
     print(rawdata[column].value_counts())

#rename the column name
insurance_data = rawdata
insurance_data =  insurance_data.rename(columns = {'Emergency dept_yes/No': 'Emergency_dept','Home or self care,':'Home_or_self_care',
                                               'Mortality risk':'Mortality_risk','Hospital County':'Hospital_County','Hospital Id':'Hospital_Id'})

# View shape of data
insurance_data.shape

# cross table for checking the impact on output variable

pd.crosstab(insurance_data.Emergency_dept,insurance_data.Result)
pd.crosstab(insurance_data.Age,insurance_data.Result)
pd.crosstab(insurance_data.Area_Service,insurance_data.Result)
pd.crosstab(insurance_data.Gender,insurance_data.Result)
pd.crosstab(insurance_data.Cultural_group,insurance_data.Result)
pd.crosstab(insurance_data.ethnicity,insurance_data.Result)
pd.crosstab(insurance_data.Admission_type,insurance_data.Result)
pd.crosstab(insurance_data.Home_or_self_care,insurance_data.Result)
pd.crosstab(insurance_data.ccs_diagnosis_code,insurance_data.Result)
pd.crosstab(insurance_data.ccs_procedure_code,insurance_data.Result)
pd.crosstab(insurance_data.apr_drg_description,insurance_data.Result)
pd.crosstab(insurance_data.Code_illness,insurance_data.Result)
pd.crosstab(insurance_data.Abortion,insurance_data.Result)
pd.crosstab(insurance_data.Mortality_risk,insurance_data.Result)
pd.crosstab(insurance_data.Surg_Description,insurance_data.Result)
pd.crosstab(insurance_data.Weight_baby,insurance_data.Result)
pd.crosstab(insurance_data.Payment_Typology,insurance_data.Result)

#taking care of missing data
insurance_data.isnull().sum()

#drop nan values in column
insurance_data = insurance_data.dropna(subset=['Hospital_Id','Mortality_risk','Area_Service','Hospital_County'],how = 'any')

#View the variabes 
insurance_data.isnull().sum()

#removes duplicates
insurance_data.drop_duplicates(keep='first',inplace=True)

insurance_data.duplicated()

# checking datatypes
insurance_data.dtypes

#Encoding the categorical variables using label encoder
 
def MultiLabelEncoder(columnlist,dataframe):
    for i in columnlist:

        labelencoder_X = LabelEncoder()
        dataframe[i] = labelencoder_X.fit_transform(dataframe[i])


columnlist = ['Area_Service','Hospital_County','Age','Gender','Days_spend_hsptl','Admission_type','Home_or_self_care','Surg_Description',
              'Emergency_dept','Cultural_group','ethnicity','apr_drg_description','Abortion']
MultiLabelEncoder(columnlist,insurance_data)

# check first few records for labeled data
insurance_data.head()

# View datatypes labeled data
insurance_data.dtypes

## "Feature Engineering"##
# splitting data for feature Engineering
X = insurance_data.loc[:, insurance_data.columns != 'Result']
Y = insurance_data.loc[: , 'Result'].values

print(X)

print(Y)

# Using extra tree classifier
from sklearn import metrics
from sklearn.ensemble import ExtraTreesClassifier
model = ExtraTreesClassifier(n_estimators=10)
model.fit(X, Y)
print(model.feature_importances_)

# Decision Tree to find feature importance in bar graph

from sklearn.datasets import make_classification
from sklearn.tree import DecisionTreeClassifier
from matplotlib import pyplot
# define the model
model = DecisionTreeClassifier()
# fit the model
model.fit(X, Y)
# get importance
importance = model.feature_importances_
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %1d, Score: %.5f' % (i,v))
# plot feature importance
pyplot.bar([x for x in range(len(importance))], importance)
pyplot.show()

# checking for features columns
X.columns

#drop insignificant fields
insurance_data = insurance_data.drop(columns=['Area_Service','Hospital_County','Gender','Cultural_group','ethnicity','Admission_type',
                                        'Surg_Description','Weight_baby','Abortion','Emergency_dept','Payment_Typology'])

# View data after dropping features
insurance_data.head()



#Standardization
scaler = StandardScaler()

insurance_data.iloc[:,0:-1] = scaler.fit_transform(insurance_data.iloc[:,0:-1].to_numpy())

# Checking data after standardization
print(insurance_data)



# Data visualization
plt.rcParams['figure.figsize'] = (20,8)

sns.histplot(insurance_data["Days_spend_hsptl"],binwidth=0.15,kde = True)

plt.show()

sns.histplot(insurance_data["Age"],kde = True,binwidth=0.15)
plt.show()

sns.histplot(insurance_data["ccs_diagnosis_code"],kde = True,binwidth=0.15)
plt.show()

sns.histplot(insurance_data["Code_illness"],kde = True,binwidth=0.15)
plt.show()

sns.histplot(insurance_data["Tot_charg"],kde = True,binwidth=1.5)
plt.show()

sns.histplot(insurance_data["Tot_cost"],kde = True,binwidth=1.6)
plt.show()

sns.histplot(insurance_data["ratio_of_total_costs_to_total_charges"],kde = True,binwidth=2.5)
plt.show()







#Calculate correlation among all variables
corr = insurance_data.corr()
pd.set_option('display.max_columns', None) 
print(corr)

#Correlation visualization among all variables
corr = insurance_data.corr()
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)
fig.colorbar(cax)
ticks = np.arange(0,len(insurance_data.columns),1)
ax.set_xticks(ticks)
plt.xticks(rotation = 90)
ax.set_yticks(ticks)
ax.set_xticklabels(insurance_data.columns)
ax.set_yticklabels(insurance_data.columns)
plt.show()

#correlation matrix visualization
corrmat = insurance_data.corr() 
  
f, ax = plt.subplots(figsize =(9, 8)) 

ax = sns.heatmap(corrmat,
                 annot=True,
                 linewidths=0.5,
                 fmt=".2f",
                 cmap="YlGnBu");
bottom, top = ax.get_ylim()
ax.set_ylim(bottom + 0.5, top - 0.5)

#covariance calculation among the all features
covMatrix = pd.DataFrame.cov(insurance_data)
pd.set_option('display.max_columns', None) 
print(covMatrix)

#chekcing for multicollinearity using variable inflation factor VIF


def calc_vif(X):

    # Calculating VIF
    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

    return(vif)

X = insurance_data.iloc[:,:-1]
calc_vif(X)

#VIF starts at 1 and has no upper limit
#VIF = 1, no correlation between the independent variable and the other variables
#VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others

# so as per the observation for multicolliniarity We can see there is no multicollinearity

# making copy of data for visualizaton purpose only
ins_data = insurance_data.copy()
ins_data = pd.DataFrame(data = np.random.random(size=(4,11)), columns = ['Hospital_Id','Age','Days_spend_hsptl','Home_or_self_care','ccs_diagnosis_code','ccs_procedure_code','Code_illness',
                                                                            'apr_drg_description','Tot_charg','Tot_cost','ratio_of_total_costs_to_total_charges'])

#visualizing data using pairplot
sns.pairplot(ins_data)

# visualizing data using boxplot
sns.boxplot(x="variable", y="value", data=pd.melt(ins_data))  #found some outliers in home_self_care,ccs_procedure_code and in code_illness

plt.show()

#Taking care of outliers using Z score method

z = np.abs(stats.zscore(insurance_data))

print(z)

# From above code it is difficult to say which data point is an outlier so we define a threshold to identify the outlier
threshold = 3
print(np.where(z > 3))

print(z[705][11])     # here we just crosscheck that 705 is row and 11 is column no. so it gives 3.22 value which is greater than 3
                      #so 705th row has an outlier

#using z score method we are removing outliers here
insurance_data = insurance_data[(z < 3).all(axis=1)]

insurance_data.shape                     #previously in data there are 1048575 rows and now 1005575 remain after removing outliers

#check for imbalance data
insurance_data['Result'].value_counts()

#0 -> fruad
# 1 -> not fraud
# visualize the target variable
g = sns.countplot(insurance_data['Result'])
g.set_xticklabels(['Fraud','Not Fraud'])
plt.show()

#we separate data using train and test set
#first we define input and output variables
y = insurance_data.Result
x = insurance_data.drop('Result',axis=1)

x.head()

y.head()

#Splitting data into train and test data
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)

#checking the shape of train and test data
print("shape of original dataset :", insurance_data.shape)
print("shape of input - training set", x_train.shape)
print("shape of output - training set", y_train.shape)
print("shape of input - testing set", x_test.shape)
print("shape of output - testing set", y_test.shape)

##Stage 1: Building the models with imbalanced dataset

#check model using xgboost classifier
# import library
import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
xgb_model = XGBClassifier().fit(x_train, y_train)
# predict
xgb_y_predict = xgb_model.predict(x_test)
# accuracy score
xgb_score = accuracy_score(xgb_y_predict, y_test)
print('Accuracy score is:', xgb_score)

"""here used XGBclassifier with imbalance data. we are getting 75% accuracy becuase predicting mostly majority class"""

##Model using random forest ensemble classifer 
# load library

from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score

from sklearn.metrics import classification_report

rfc = RandomForestClassifier()
#fit the predictor and target
rfc.fit(x_train, y_train)
# predict

rfc_predict = rfc.predict(x_test)# check performance
print('ROCAUC score:',roc_auc_score(y_test, rfc_predict))
print('Accuracy score:',accuracy_score(y_test, rfc_predict))
print('F1 score:',f1_score(y_test, rfc_predict))
# print classification report 
predictions = rfc.predict(x_test)
print(classification_report(y_test, predictions))

"""using random forest ensemble method recall n f1 score is improved at majority class only ,along with that model accuracy is 73%"""

# logistic regression model
lr = LogisticRegression() 
  
# train the model on train set 
lr.fit(x_train, y_train.ravel()) 
  
predictions = lr.predict(x_test) 
  
# print classification report 
print(classification_report(y_test, predictions))

"""we can see here f1 score for minority class is 0 and for majority class is 0.86 which is very huge difference between both classes and model accuracy is very less 75%"""

##Stage 2: Model building with Balanced data
# Data balancing using SMOTE oversampling technique
print("Before OverSampling, counts of label '1': {}".format(sum(y_train == 1))) 
print("Before OverSampling, counts of label '0': {} \n".format(sum(y_train == 0))) 
  
# import SMOTE module from imblearn library 
# pip install imblearn (if you don't have imblearn in your system) 

sm = SMOTE(random_state = 2) 
x_train_res, y_train_res = sm.fit_sample(x_train, y_train.ravel()) 
  
print('After OverSampling, the shape of train_x: {}'.format(x_train_res.shape)) 
print('After OverSampling, the shape of train_y: {} \n'.format(y_train_res.shape)) 
  
print("After OverSampling, counts of label '1': {}".format(sum(y_train_res == 1))) 
print("After OverSampling, counts of label '0': {}".format(sum(y_train_res == 0)))

#model test after data balancing using logistic regression
#prediction and recall
lr1 = LogisticRegression() 
lr1.fit(x_train_res, y_train_res.ravel()) 
predictions = lr1.predict(x_test) 
  
# print classification report 
print(classification_report(y_test, predictions))

"""using SMOTE algorithm checked accuracy again but recall is bit adjust but still not good output using oversampling. accuracy is very low 47%

"""

##Stage 3: data split using Startified fold for data balancing  
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.model_selection import StratifiedKFold

print('No Frauds', round(insurance_data['Result'].value_counts()[1]/len(insurance_data) * 100,2), '% of the dataset')
print('Frauds', round(insurance_data['Result'].value_counts()[0]/len(insurance_data) * 100,2), '% of the dataset')

X = insurance_data.drop('Result', axis=1)
y = insurance_data['Result']

sss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)

for train_index, test_index in sss.split(X, y):
    print("Train:", train_index, "Test:", test_index)
    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]
    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]
# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.
# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)

# Check the Distribution of the labels


# Turn into an array
original_Xtrain = original_Xtrain.values
original_Xtest = original_Xtest.values
original_ytrain = original_ytrain.values
original_ytest = original_ytest.values

# See if both the train and test label distribution are similarly distributed
train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)
test_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)
print('-' * 100)

print('Label Distributions: \n')
print(train_counts_label/ len(original_ytrain))
print(test_counts_label/ len(original_ytest))

# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.
# shuffle the data before creating the subsamples
#Using Random Undersampling
insurance_data = insurance_data.sample(frac=0.1)

# amount of fraud classes 251896 rows.
fraud_data = insurance_data.loc[insurance_data['Result'] == 0]
non_fraud_data = insurance_data.loc[insurance_data['Result'] == 1][:251896]

normal_dist_data = pd.concat([fraud_data, non_fraud_data])

# Shuffle dataframe rows
new_insurance = normal_dist_data.sample(frac=0.01, random_state=42)

new_insurance.head()

# Data balancing visualization using countplot
import itertools 
print('Distribution of the Classes in the subsample dataset')
print(new_insurance['Result'].value_counts()/len(new_insurance))

sns.color_palette("pastel")
sns.countplot('Result', data=new_insurance)
plt.title('Equally Distributed Classes', fontsize=14)
plt.show()

# Undersampling before cross validating (prone to overfit)
X = new_insurance.drop('Result', axis=1)
y = new_insurance['Result']

from sklearn.model_selection import train_test_split

# This is explicitly used for undersampling.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Turn the values into an array for feeding the classification algorithms.
X_train = X_train.values
X_test = X_test.values
y_train = y_train.values
y_test = y_test.values

# implement simple classifiers
from sklearn.tree import DecisionTreeClassifier
classifiers = {
    "LogisiticRegression": LogisticRegression(),
     "Random Forest": RandomForestClassifier(),
    "DecisionTreeClassifier": DecisionTreeClassifier()
}

#Evaluating models using cross validation
from sklearn.model_selection import cross_val_score


for key, classifier in classifiers.items():
    classifier.fit(X_train, y_train)
    training_score = cross_val_score(classifier, X_train, y_train, cv=5)
    print("Classifiers: ", classifier.__class__.__name__, "Has a training score of", round(training_score.mean(), 2) * 100, "% accuracy score")

"""so by doing downsampling here we got very low accuracy for all three classifier"""

###Stage 4: To improve the accuracy of model Upsampling is used

#Upsampling: Random forest classifier
#Divinding data into majority n minority class
data_minority = new_insurance.loc[insurance_data['Result']==0]
data_majority = new_insurance.loc[insurance_data['Result']==1]

#Resampling 
from sklearn.utils import resample
data_minority_upsampled = resample(data_minority, replace=True, n_samples=500, random_state=42)

data_upsampled = pd.concat([data_majority, data_minority_upsampled ], ignore_index=True)

X = data_upsampled.drop(['Result'],axis=1).values
Y = data_upsampled.Result.values

#Splitting data into train and test set
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, stratify=Y, test_size=0.3, random_state=42)

#Buiding Random Forest model using upsampling
from sklearn.ensemble import RandomForestClassifier
model_RF_upsampled = RandomForestClassifier(n_estimators=50, random_state=42)

model_RF_upsampled.fit(X_train, Y_train)

#Classification report for train data
from sklearn.metrics import classification_report, confusion_matrix
Y_pred = model_RF_upsampled.predict(X_train)

print(classification_report(Y_train, Y_pred))

#classification report for test data
Y_pred_test = model_RF_upsampled.predict(X_test)
print(classification_report(Y_test, Y_pred_test))

#Building model pickle file for flask
import pickle
pickle.dump(model_RF_upsampled,open('C:/Users/Ashmita Agrawal/Desktop/Learning/Deployment of project using Flask/model.pkl','wb'))
model = pickle.load(open('C:/Users/Ashmita Agrawal/Desktop/Learning/Deployment of project using Flask/model.pkl','rb'))
labelencoder_X = LabelEncoder()
final_features = np.array([37,'30 to 49',4,'Home Care',122,0,'Medical',1,1,5511.95,5582.49,1.0128])
for i in final_features:
    final_features= labelencoder_X.fit_transform(final_features)
    print(final_features)
scaler = StandardScaler()
final = pd.DataFrame(final_features)
final_features = np.array([final_features])
print(model.predict(final_features))


#confusion matrix for test data
print(confusion_matrix(Y_test, Y_pred_test))

#classification report for whole data
Y_pred_whole = model_RF_upsampled.predict(insurance_data.drop(['Result'],axis=1))
 
print(classification_report(insurance_data.Result.values, Y_pred_whole))

##confusion matrix for whole data
print(confusion_matrix(insurance_data.Result.values, Y_pred_whole))

# Evaluating model using startified fold
from sklearn.model_selection import StratifiedKFold 
# Create StratifiedKFold object. 
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) 
lst_accu_stratified = [] 
   
for train_index, test_index in skf.split(X, Y): 
    x_train_fold, x_test_fold = X[train_index], X[test_index] 
    y_train_fold, y_test_fold = Y[train_index], Y[test_index] 
    model_RF_upsampled.fit(x_train_fold, y_train_fold) 
    lst_accu_stratified.append(model_RF_upsampled.score(x_test_fold, y_test_fold)) 
   
# Print the output. 
print('List of possible accuracy:', lst_accu_stratified) 
print('\nMaximum Accuracy That can be obtained from this model is:', 
      max(lst_accu_stratified)*100, '%') 
print('\nMinimum Accuracy:', 
      min(lst_accu_stratified)*100, '%') 
print('\nOverall Accuracy:', 
      np.mean(lst_accu_stratified)*100, '%') 
print('\nStandard Deviation is:', np.std(lst_accu_stratified))

#confusion matrix visualization using heatmap
sns.heatmap(confusion_matrix(insurance_data.Result.values, Y_pred_whole), annot=True, fmt='.7g' )
plt.show()

#Saving the model
from joblib import dump, load
dump(model_RF_upsampled, 'model_RF_upsampled.joblib')

## Building Decision tree classifier using upsampling
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
dt_upsampled = DecisionTreeClassifier(random_state=0)

dt_upsampled.fit(X_train, Y_train)

#classification report for train data
dt_y_pred = dt_upsampled.predict(X_train)

print(classification_report(Y_train, dt_y_pred))

#classification report for test data
dt_y_pred_test = dt_upsampled.predict(X_test)

print(classification_report(Y_test, dt_y_pred_test))

#confusion matrix for test data
print(confusion_matrix(Y_test, dt_y_pred_test))

##classification report for whole data
dt_Y_pred_whole = dt_upsampled.predict(insurance_data.drop(['Result'],axis=1))

print(classification_report(insurance_data.Result.values, dt_Y_pred_whole))

#confusion matrix for whole data
print(confusion_matrix(insurance_data.Result.values, dt_Y_pred_whole))

#Evaluating model using stratified fold method
from sklearn.model_selection import StratifiedKFold 
# Create StratifiedKFold object. 
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) 
lst_accu_stratified = [] 
   
for train_index, test_index in skf.split(X, Y): 
    x_train_fold, x_test_fold = X[train_index], X[test_index] 
    y_train_fold, y_test_fold = Y[train_index], Y[test_index] 
    dt_upsampled.fit(x_train_fold, y_train_fold) 
    lst_accu_stratified.append(dt_upsampled.score(x_test_fold, y_test_fold)) 
   
# Print the output. 
print('List of possible accuracy:', lst_accu_stratified) 
print('\nMaximum Accuracy That can be obtained from this model is:', 
      max(lst_accu_stratified)*100, '%') 
print('\nMinimum Accuracy:', 
      min(lst_accu_stratified)*100, '%') 
print('\nOverall Accuracy:', 
      np.mean(lst_accu_stratified)*100, '%') 
print('\nStandard Deviation is:', np.std(lst_accu_stratified))

"""> **`Stratified Cross Valiadtion`**: 
    Splits the data into k folds, making sure each fold is an appropriate representative of the original data. (class distribution, mean, variance, etc)



"""

sns.heatmap(confusion_matrix(insurance_data.Result.values, dt_Y_pred_whole), annot=True, fmt='.7g' )
plt.show()

"""Decision tree classifier gives final accuracy **83.67%**"""

from joblib import dump, load
dump(dt_upsampled, 'dt_upsampled.joblib')

#xgboost model using upsampling
from sklearn.ensemble import GradientBoostingClassifier
xgboost_upsampled = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
      max_depth=1, random_state=0).fit(X_train, Y_train)

#classification report for train data 
xgb_y_pred = xgboost_upsampled.predict(X_train)

print(classification_report(Y_train, xgb_y_pred))

#classification report for test data
xgb_y_pred_test = xgboost_upsampled.predict(X_test)

print(classification_report(Y_test, xgb_y_pred_test))

"""upsampling or downsampling in xgboost it has same accuracy in both balancing methods"""

### xgboost model using downsampling
data_minority = insurance_data.loc[insurance_data['Result']==0]
data_majority = insurance_data.loc[insurance_data['Result']==1]

#Resampling 
data_majority_downsampled = resample(data_majority, replace=False, n_samples=251896, random_state=42)

data_balanced = pd.concat([data_majority_downsampled, data_minority ], ignore_index=True)

X = data_balanced.drop(['Result'],axis=1).values
Y = data_balanced.Result.values

#Splitting data in train and test data
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, stratify=Y, test_size=0.3, random_state=42)

from sklearn.ensemble import GradientBoostingClassifier
xgboost_downsampled = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
      max_depth=1, random_state=0).fit(X_train, Y_train)

#classification report for train data
xboost_y_pred = xgboost_downsampled.predict(X_train)

print(classification_report(Y_train, xboost_y_pred))

#classification report for test data
y_pred_test = xgboost_downsampled.predict(X_test)

print(classification_report(Y_test, y_pred_test))

#confusion matrix for test data 
print(confusion_matrix(Y_test, y_pred_test))

#classification report for whole data
y_pred_whole = xgboost_downsampled.predict(insurance_data.drop(['Result'],axis=1))

print(classification_report(insurance_data.Result.values, y_pred_whole))

#confusion matrix for whole data
print(confusion_matrix(insurance_data.Result.values, y_pred_whole))

#Evaluating model using StratifiedKFold
from sklearn.model_selection import StratifiedKFold 
# Create StratifiedKFold object. 
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) 
lst_accu_stratified = [] 
   
for train_index, test_index in skf.split(X, Y): 
    x_train_fold, x_test_fold = X[train_index], X[test_index] 
    y_train_fold, y_test_fold = Y[train_index], Y[test_index] 
    xgboost_downsampled.fit(x_train_fold, y_train_fold) 
    lst_accu_stratified.append(xgboost_downsampled.score(x_test_fold, y_test_fold)) 
   
# Print the output. 
print('List of possible accuracy:', lst_accu_stratified) 
print('\nMaximum Accuracy That can be obtained from this model is:', 
      max(lst_accu_stratified)*100, '%') 
print('\nMinimum Accuracy:', 
      min(lst_accu_stratified)*100, '%') 
print('\nOverall Accuracy:', 
      np.mean(lst_accu_stratified)*100, '%') 
print('\nStandard Deviation is:', np.std(lst_accu_stratified))

"""With downsampling xgboost gives very less accuracy i.e. 50% only"""





# importing sweetviz
import sweetviz as sv

#analyzing the dataset
EDA_report = sv.analyze(insurance_data)

#display the report
EDA_report.show_html('/content/drive/MyDrive/Colab Notebooks/insurance_data.html');

